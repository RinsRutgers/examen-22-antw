{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.settings import StyleSettings\n",
    "from src.data.data_tools import StyleDataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = StyleSettings()\n",
    "traindataset = StyleDataset([settings.trainpath])\n",
    "testdataset = StyleDataset([settings.testpath])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 419 batches in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset) // 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Lace is an openwork fabric , patterned with open holes in the work , made by machine or by hand.',\n",
       " 'wiki')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = traindataset[42]\n",
    "x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every batch is a `Tuple[str, str]` of a sentence and a label. We can see this is a classification task.\n",
    "The task is, to classify sentences in four categories.\n",
    "Lets build a vocabulary by copy-pasting the code we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:10:08.758 | INFO     | src.models.tokenizer:build_vocab:27 - Found 19306 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19308"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import tokenizer\n",
    "\n",
    "corpus = []\n",
    "count = []\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][0])\n",
    "    corpus.append(x)\n",
    "    \n",
    "v = tokenizer.build_vocab(corpus, max=20000)\n",
    "len(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast the labels to an integers. You can use this dictionary to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"humor\": 0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "d[y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Figure out, for every class, what accuracy you should expect if the model would guess blind on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humor komt 31.414510476474533 % voor in de dataset, blind guess accuracy for this class: 7.853627619118633\n",
      "reuters komt 31.21318320781448 % voor in de dataset, blind guess accuracy for this class: 7.80329580195362\n",
      "wiki komt 31.175900380284844 % voor in de dataset, blind guess accuracy for this class: 7.793975095071211\n",
      "proverbs komt 6.196405935426143 % voor in de dataset, blind guess accuracy for this class: 1.5491014838565358\n"
     ]
    }
   ],
   "source": [
    "count = {\"humor\": 0, \"reuters\": 0, \"wiki\": 0, \"proverbs\": 0}\n",
    "total = len(traindataset)\n",
    "for i in range(len(traindataset)):\n",
    "    x = tokenizer.clean(traindataset[i][1])\n",
    "    count[x] = count[x] + 1\n",
    "\n",
    "for style in d:\n",
    "    print(f'{style} komt {((count[style] / total ) * 100 )} % voor in de dataset, blind guess accuracy for this class: {((count[style] / total ) * 100 )*.25}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflect on what you see. What does this mean? What implications does this have? Why is that good/bad?\n",
    "Are there things down the line that could cause a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proverbs komt erg weining voor, daarom heeft het model weinig om daar van te leren. De nuance mist dan gok ik en je model wordt biased. \n",
    "\n",
    "ik dacht dat je de rest van de dataset kon trimmen tot een gelijwaardige verdeling, maar dit kan niet zo makkelijk zoals ik dacht omdat je inderdaad nog shuffled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Implement a preprocessor\n",
    "\n",
    "We can inherit from `tokenizer.Preprocessor`\n",
    "Only thing we need to adjust is the `cast_label` function.\n",
    " \n",
    "- create a StylePreprocessor class\n",
    "- inherit from Preprocessor\n",
    "- create a new cast_label function for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ~ about 4 lines of code\n",
    "\n",
    "class StylePreprocessor(tokenizer.Preprocessor):\n",
    "    def cast_label(self, label: str) -> int:\n",
    "        d = {\"humor\":0, \"reuters\": 1, \"wiki\": 2, \"proverbs\": 3}\n",
    "        return d[label]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1347]], dtype=torch.int32), tensor([2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = StylePreprocessor(max=100, vocab=v, clean=tokenizer.clean)\n",
    "preprocessor([(x, y)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model\n",
    "We can re-use the BaseDatastreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data_tools\n",
    "\n",
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 57]),\n",
       " tensor([0, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 1, 2, 0, 0, 3, 0, 2, 1, 0, 1,\n",
       "         0, 1, 1, 2, 1, 0, 0, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(trainstreamer)\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Metrics, loss\n",
    "Select proper metrics and a loss function.\n",
    "\n",
    "Bonus: implement an additional metric function that is relevant for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "import torch\n",
    "# TODO ~ 2 lines of code\n",
    "\n",
    "metrics = [metrics.Accuracy(), metrics.F1Score()]\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is een handige metric als je data goed evenredig versprijd is. Dat is hiet niet het geval. Voor ongebalanceerde datasets wordt F1score aangeraden. Bron: https://towardsdatascience.com/the-f1-score-bec2bbc38aa6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Basemodel\n",
    "Create a base model. It does not need to be naive; you could re-use the\n",
    "NLP models we used for the IMDB.\n",
    "\n",
    "I suggest to start with a hidden size of about 128.\n",
    "Use a config dictionary, or a gin file, both are fine.\n",
    "\n",
    "Bonus points if you create a Trax model in src.models, and even more if you add a trax training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPmodel(\n",
      "  (emb): Embedding(19308, 128)\n",
      "  (rnn): GRU(128, 128, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (linear): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "log_dir = settings.log_dir\n",
    "# TODO between 2 and 8 lines of code, depending on your setup\n",
    "# Assuming you load your model in one line of code from src.models.rnn\n",
    "from src.models import rnn\n",
    "\n",
    "config = {\n",
    "    \"vocab\": len(v),\n",
    "    \"input_size\": 32,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"output_size\": 4,\n",
    "}\n",
    "\n",
    "model = rnn.NLPmodel(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the trainloop.\n",
    "\n",
    "- Give the lenght of the traindataset, how many batches of 32 can you get out of it?\n",
    "- If you take a short amount of train_steps (eg 25) for every epoch, how many epochs do you need to cover the complete dataset?\n",
    "- What amount of epochs do you need to run the loop with trainsteps=25 to cover the complete traindataset once? \n",
    "- answer the questions above, and pick a reasonable epoch lenght\n",
    "\n",
    "Start with a default learning_rate of 1e-3 and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419.09375 batches in traindataset\n",
      "16.76375 epochs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(traindataset) / 32} batches in traindataset\")\n",
    "print(f\"{(len(traindataset) / 32)/25} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opmerking\n",
    "\n",
    "Hoi Raoul, bij bovenstaande vraag vraag ik mij af of de term epochs wel helemaal juist is, ik lees overal op het internet dat een epoch betekend dat je een keer door de hele dataset gaat. Maar als ik naar de vraag kijk lijkt dat niet helemaal op te gaan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.training import train_model\n",
    "\n",
    "# model = train_model.trainloop(\n",
    "#     epochs=64,\n",
    "#     model=model,\n",
    "#     metrics=metrics,\n",
    "#     optimizer=torch.optim.Adam,\n",
    "#     learning_rate=1e-3,\n",
    "#     loss_fn=loss_fn,\n",
    "#     train_dataloader=trainstreamer,\n",
    "#     test_dataloader=teststreamer,\n",
    "#     log_dir=\"../model/basis\",\n",
    "#     train_steps=25,\n",
    "#     eval_steps=25,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save an image from the training in tensorboard in the `figures` folder.\n",
    "Explain what you are seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src = \"../figures/Screenshot 2022-07-21 at 16.01.07.png\">\n",
    "\n",
    "## Antwoord\n",
    "Je ziet dat het model leert en de loss op de train data flink minder wordt, dit zie je ook terug op de accuracy. Alleen de accuracy op de test data stagneert rond de 0.3, wat overeenkomt met de F1 score, wat aangeeft dat de F1 score hier dus echt een betere metric is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluate the basemodel\n",
    "Create a confusion matrix with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 15.0, 'Predicted'), Text(33.0, 0.5, 'Target')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _ in range(100):\n",
    "    X, y = next(teststreamer)\n",
    "    yhat = model(X)\n",
    "    yhat = yhat.argmax(dim=1)\n",
    "    y_pred.append(yhat.tolist())\n",
    "    y_true.append(y.tolist())\n",
    "\n",
    "yhat = [x for y in y_pred for x in y]\n",
    "y = [x for y in y_true for x in y]\n",
    "\n",
    "cfm = confusion_matrix(y, yhat)\n",
    "cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "plot = sns.heatmap(cfm_norm, annot=cfm_norm, fmt=\".3f\")\n",
    "plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this in the figures folder.\n",
    "Interpret this. \n",
    "\n",
    "<img src = \"../figures/matrix_2.png\">\n",
    "\n",
    "- What is going on?\n",
    "    \n",
    "- What is a good metric here?\n",
    "    \n",
    "- how is your answer to Q1 relevant here?\n",
    "    \n",
    "- Is there something you could do to fix/improve things, after you see these results?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als je de resultaten in losse klasses bekijkt wordt dijdelijk dat het model niet goed scoort bij klasse 3, in bijna 30 proces van de gevallen voorspelt het een andere klasse, vooral 0 en 2. Dit terwijl de andere klasse wel goed scoren. Niet geheel onlogisch want het model heeft in 6% van de tijd maar voorbeelden gezien van klasse 3. \n",
    "\n",
    "Accuracy is hier dus geen goede score omdat de dataset zo onevenredig verdeeld is. Gebruik maken van F1 score is al een goede eerste zet omdat je dan gebruik maakt van de precision en recall van alle classe in een metric. \n",
    "\n",
    "Je kunt als tweede oplossing resamplen, zodat de dataset gebalanceerder is. En dit kan op drie manieren: \n",
    "-   undersamping (van klasse 0-2 afhalen) niet handig denk ik want dan verlies je echt mega veel informatie. \n",
    "-   oversampling (delen van klasse 3 dupliceren), ook onhandig denk ik ivm overfitten. \n",
    "-   creeeren van nieuwe synthetic data. Waarbij ik niet denk dat je de laatste kunt gebruiken bij NLP. Althans, nee.. lijkt me onmogelijk. \n",
    "\n",
    "En je kunt natuurlijk het model aanpassen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tune the model\n",
    "Don't overdo this.\n",
    "More is not better.\n",
    "\n",
    "Bonus points for things like:\n",
    "- Attention layers\n",
    "- Trax architecture including a functioning training loop\n",
    "\n",
    "Keep it small! It's better to present 2 or 3 sane experiments that are structured and thought trough, than 25 random guesses. You can test more, but select 2 or 3 of the best alternatives you researched, with a rationale why this works better.\n",
    "\n",
    "Keep it concise; explain:\n",
    "- what you changed\n",
    "- why you thought that was a good idea  \n",
    "- what the impact was (visualise or numeric)\n",
    "- explain the impact\n",
    "\n",
    "You dont need to get a perfect score; curiousity driven research that fails is fine.\n",
    "The insight into what is happening is more important than the quantity.\n",
    "\n",
    "Keep logs of your settings;\n",
    "either use gin, or save configs, or both :)\n",
    "Store images in the `figures` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainstreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=traindataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()\n",
    "teststreamer = data_tools.BaseDatastreamer(\n",
    "    dataset=testdataset, batchsize=32, preprocessor=preprocessor\n",
    ").stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:52:04.103 | INFO     | src.data.data_tools:dir_add_timestamp:67 - Logging to ../tune/20220804-1252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     56   128  256  512\n",
      "0.1  NaN  NaN  NaN  NaN\n",
      "0.2  NaN  NaN  NaN  NaN\n",
      "0.3  NaN  NaN  NaN  NaN\n",
      "0.4  NaN  NaN  NaN  NaN\n",
      "NLPmodel(\n",
      "  (emb): Embedding(19308, 56)\n",
      "  (rnn): GRU(56, 56, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (linear): Linear(in_features=56, out_features=4, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00,  9.65it/s]\n",
      "2022-08-04 12:52:07.271 | INFO     | src.training.train_model:trainloop:164 - Epoch 0 train 1.2926 test 1.2483 metric ['0.4138', '0.2659']\n",
      "100%|██████████| 25/25 [00:01<00:00, 15.10it/s]\n",
      "2022-08-04 12:52:09.368 | INFO     | src.training.train_model:trainloop:164 - Epoch 1 train 1.2424 test 1.2351 metric ['0.3762', '0.2429']\n",
      "100%|██████████| 25/25 [00:01<00:00, 14.53it/s]\n",
      "2022-08-04 12:52:11.543 | INFO     | src.training.train_model:trainloop:164 - Epoch 2 train 1.2260 test 1.2065 metric ['0.4175', '0.2534']\n",
      "100%|██████████| 25/25 [00:02<00:00, 12.12it/s]\n",
      "2022-08-04 12:52:14.134 | INFO     | src.training.train_model:trainloop:164 - Epoch 3 train 1.2434 test 1.1943 metric ['0.4225', '0.2455']\n",
      "100%|██████████| 25/25 [00:01<00:00, 13.06it/s]\n",
      "2022-08-04 12:52:16.725 | INFO     | src.training.train_model:trainloop:164 - Epoch 4 train 1.1614 test 1.2589 metric ['0.3800', '0.2853']\n",
      " 44%|████▍     | 11/25 [00:00<00:00, 16.64it/s]\n",
      "  8%|▊         | 5/64 [00:13<02:36,  2.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39mNLPmodel(config)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=22'>23</a>\u001b[0m model \u001b[39m=\u001b[39m train_model\u001b[39m.\u001b[39;49mtrainloop(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=23'>24</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=24'>25</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=25'>26</a>\u001b[0m     metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=26'>27</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=27'>28</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=28'>29</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=29'>30</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrainstreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=30'>31</a>\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mteststreamer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=31'>32</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49mlog_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=32'>33</a>\u001b[0m     train_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=33'>34</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=34'>35</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d6173746572536572766572227d/home/rinsrutgers/examen-22-antw/notebooks/02_style_detection.ipynb#ch0000040vscode-remote?line=35'>36</a>\u001b[0m result\u001b[39m.\u001b[39mat[x,i] \u001b[39m=\u001b[39m  model[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-KwCzH9GQ-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/examen-22-antw/notebooks/../src/training/train_model.py:139\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m    136\u001b[0m     write_gin(log_dir)\n\u001b[1;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 139\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m    140\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     metric_dict, test_loss \u001b[39m=\u001b[39m evalbatches(\n\u001b[1;32m    144\u001b[0m         model, test_dataloader, loss_fn, metrics, eval_steps\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(test_loss)\n",
      "File \u001b[0;32m~/examen-22-antw/notebooks/../src/training/train_model.py:42\u001b[0m, in \u001b[0;36mtrainbatches\u001b[0;34m(model, traindatastreamer, loss_fn, optimizer, train_steps)\u001b[0m\n\u001b[1;32m     40\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(traindatastreamer))\n\u001b[1;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m yhat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     43\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(yhat, y)\n\u001b[1;32m     44\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-KwCzH9GQ-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/examen-22-antw/notebooks/../src/models/rnn.py:27\u001b[0m, in \u001b[0;36mNLPmodel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     26\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(x)\n\u001b[0;32m---> 27\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x)\n\u001b[1;32m     28\u001b[0m     last_step \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     29\u001b[0m     yhat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(last_step)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-KwCzH9GQ-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/exam-22-KwCzH9GQ-py3.9/lib/python3.9/site-packages/torch/nn/modules/rnn.py:942\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    941\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    943\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    944\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    946\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "hidden_size_array = (56, 128, 256, 512)\n",
    "dropout_array = (0.1, 0.2, 0.3, 0.4)\n",
    "\n",
    "result = pd.DataFrame(index = dropout_array, columns=hidden_size_array)\n",
    "print(result)\n",
    "\n",
    "for i in hidden_size_array:\n",
    "    for x in dropout_array:\n",
    "\n",
    "        config = {\n",
    "            \"vocab\": len(v),\n",
    "            \"input_size\": 32,\n",
    "            \"hidden_size\": i,\n",
    "            \"num_layers\": 3,\n",
    "            \"dropout\": x,\n",
    "            \"output_size\": 4,\n",
    "        }\n",
    "\n",
    "        model = rnn.NLPmodel(config)\n",
    "        print(model)\n",
    "\n",
    "        model = train_model.trainloop(\n",
    "            epochs=64,\n",
    "            model=model,\n",
    "            metrics=metrics,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            learning_rate=1e-3,\n",
    "            loss_fn=loss_fn,\n",
    "            train_dataloader=trainstreamer,\n",
    "            test_dataloader=teststreamer,\n",
    "            log_dir=log_dir,\n",
    "            train_steps=25,\n",
    "            eval_steps=25,\n",
    "        )\n",
    "        result.at[x,i] =  model[1]\n",
    "        print(result)\n",
    "\n",
    "\n",
    "        # result[][str(x)] = model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      56.0  128.0 256.0 512.0   0.1     0.2     0.3     0.4  \n",
      "0.1     NaN   NaN   NaN   NaN     NaN     NaN     NaN     NaN\n",
      "0.2     NaN   NaN   NaN   NaN     NaN     NaN     NaN     NaN\n",
      "0.3     NaN   NaN   NaN   NaN     NaN     NaN     NaN     NaN\n",
      "0.4     NaN   NaN   NaN   NaN     NaN     NaN     NaN     NaN\n",
      "56.0    NaN   NaN   NaN   NaN  0.7746  0.8195  0.7847  0.8370\n",
      "128.0   NaN   NaN   NaN   NaN  0.8566  0.8720  0.8326     NaN\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('exam-22-KwCzH9GQ-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "793d4b0ba659e9ff3f4cb7a1b3752627ccc54f1224ce4526b6cf626154c05563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
